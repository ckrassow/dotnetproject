{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "IMAGE_DIR = \"./data/images/\"\n",
    "os.chdir(IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"UniversitySchoolProjectImageScraper/1.0 corkr933@student.liu.se\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, headers=headers)\n",
    "        response.raise_for_status()  \n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            for block in response.iter_content(1024):\n",
    "                if not block:\n",
    "                    break\n",
    "                f.write(block)\n",
    "\n",
    "        print(f\"Image downloaded: {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cc_info(file_url):\n",
    "    response = requests.get(f\"https://en.wikipedia.org{file_url}\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    user_element = soup.find('a', title=lambda t: t and t.startswith('User:'))\n",
    "    username = user_element.text.strip() if user_element else \"Unknown\"\n",
    "    license_info = \"CC_BY-SA_4.0\"\n",
    "    return username, license_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player_images(url, player_name):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    image_links = soup.find_all('a', class_='mw-file-description')\n",
    "    if \" (captain)\" in player_name:\n",
    "        player_name = player_name.replace(\" (captain)\", \"\")\n",
    "        \n",
    "    if not image_links:  \n",
    "        print(f\"No image found for {player_name}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    for image_link in image_links:\n",
    "        image_tag = image_link.find('img', class_='mw-file-element')\n",
    "        if image_tag:\n",
    "            image_url = image_tag['src']\n",
    "            if image_url.startswith(\"//\"):\n",
    "                image_url = \"https:\" + image_url \n",
    "            \n",
    "            file_url = image_link['href']\n",
    "            username, license_info = extract_cc_info(file_url)\n",
    "\n",
    "            filename = f\"{player_name}_{username}_{license_info}.jpg\"\n",
    "            filename = filename.replace(\" \", \"_\")  \n",
    "            filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "            if os.path.isfile(os.path.join(IMAGE_DIR, filename)):\n",
    "                print(f\"Image for {player_name} already exists. Skipping...\")\n",
    "            else:\n",
    "                download_image(image_url, filename)\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team_images(url, team_name):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    image_links = soup.find_all('a', class_='mw-file-description')\n",
    "        \n",
    "    if not image_links:  \n",
    "        print(f\"No image found for {team_name}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    for image_link in image_links:\n",
    "        image_tag = image_link.find('img', class_='mw-file-element')\n",
    "        if image_tag:\n",
    "            image_url = image_tag['src']\n",
    "            if image_url.startswith(\"//\"):\n",
    "                image_url = \"https:\" + image_url \n",
    "            \n",
    "            file_url = image_link['href']\n",
    "            username, license_info = extract_cc_info(file_url)\n",
    "\n",
    "            filename = f\"{team_name}_{username}_{license_info}.jpg\"\n",
    "            filename = filename.replace(\" \", \"_\")  \n",
    "            filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "            if os.path.isfile(os.path.join(IMAGE_DIR, filename)):\n",
    "                print(f\"Image for {team_name} already exists. Skipping...\")\n",
    "            else:\n",
    "                download_image(image_url, filename)\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    #for _, row in df.iterrows():\n",
    "    #    player_name = row['Player']\n",
    "    #    url_to_scrape = f\"https://en.wikipedia.org/wiki/{player_name.replace(' ', '_')}\"  \n",
    "    #    scrape_player_images(url_to_scrape, player_name)\n",
    "    for _, row in df.iterrows():\n",
    "        team_name = row['Team']\n",
    "        url_to_scrape = f\"https://en.wikipedia.org/wiki/{team_name.replace(' ', '_')}_national_football_team\"\n",
    "        scrape_team_images(url_to_scrape, team_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded: Germany_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Scotland_Foxtrot1985_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Hungary_Thommy_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Switzerland_User_Marc_Mongenet_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Spain_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Croatia_Inkwina_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Italy_Unknown_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Albania_Xhulianoo_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Slovenia_Minorax_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Denmark_Unknown_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Serbia_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: England_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Poland_Denelson83_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Netherlands_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Austria_Anomie_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: France_Nathanlg94_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Belgium_DatBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Slovakia_S.A._Julio_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Romania_RandyFitz_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Ukraine_Chabe01_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Turkey_ANGELUS_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Georgia_Unknown_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Portugal_RonBot_CC_BY-SA_4.0.jpg\n",
      "Image downloaded: Czech_Republic_ThecentreCZ_CC_BY-SA_4.0.jpg\n"
     ]
    }
   ],
   "source": [
    "csv_filename = \"../euro_team_data.csv\"  \n",
    "scrape_images_from_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
